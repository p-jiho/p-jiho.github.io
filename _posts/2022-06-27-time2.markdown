---
layout : post
title : CNN과 RNN
date: 2022-06-27
catagories : 
  - Deep learning
tags : 
  - Deep learning
last_modified_at: 2022-06-27
---

## 1. CNN

<br>

## CNN이란?
CNN은 Convolutional Neural Network의 약자이다.

즉, 합성곱 연산을 하는 신경망이다.

이미지 데이터에 딥러닝을 적용할 수 있다.

하지만 DNN을 이용하여 이미지를 일자로 펼쳐서 입력을 한다면 이미지의 해상도가 높은 경우 파라미터의 수가 너무 많아진다.

이로인해 학습속도 저하, 수렴여부 불분명 등 문제들이 생긴다.

또한, 이미지의 기하학적인 공간적 정보를 유지할 수가 없다.

일자로 펼친다면 같은 이미지이지만 크기나 모양이 조금만 달라도 다른 이미지가 되어버리므로 공간적 정보를 유지하는 것이 중요하다.

CNN의 주요한 특정은 이미지 전체를 보기보다는 부분부분을 보고, 주변 픽셀들과의 연관성의 살리는 것이다.

이로인해 위치가 달라도 크기가 달라도 같은 이미지로 판별할 수 있다.


<br><br>

## CNN의 구조
컬러 이미지의 경우 3D로 표현이 된다. 채널의 크기는 3이고 각각 R, G, B 값이 저장이 되어 컬러로 나타난다.

흑백 이미지의 경우에는 2D로 표현이 된다. 채널의 크기는 1이다.

<br><br>

## Convolution Layer
Convolution Layer에서는 filter을 사용한다.

filter을 이용해 합성곱 연산을 한다. 옆으로 한칸씩 이동해가며 2X2 or 3X3과 같은 filter를 이용해 합성곱 연산을 한다.

![합성곱연산](https://user-images.githubusercontent.com/76985302/175927986-7878e71e-81c4-4bda-874d-988aa9b39052.PNG)

컬러 이미지의 경우는 filter도 3차원을 가진다.

또한, Stride를 조절해 한칸씩 움직일 것인지 두칸씩 움직일 것인지 결정해주어야한다.

또한, Activation function을 사용하게 되는데 Sigmoid, Tanh, ReLU, SELU 등이 있다.

전통적으로는 Sigmoid, Tanh을 사용했으나 Layer가 깊어질 수록 Gradient weight들이 0으로 수렴하는 문제가 생긴다.

이는 초기 layer의 파라미터 값이 크게 변해도 output에는 영향을 주지 못한다. 즉, 처음의 layer 값을 제대로 반영하지 못하게 된다.

이 Vanishing gradient 문제를 해결한 것이 ReLU이다. 

이 비선형 활성화 함수를 적용하는 이유는 선형함수의 layer을 아무리 쌓아봤자 선형함수의 연산이기 때문이다.

<br><br>

## Padding

Padding은 합성곱 연산을 할 때, 출력 이미지의 크기보다 입력 이미지의 크기가 작아진다.

이로인해 filter을 이용할수록 이미지의 가장자리에 있는 정보들은 사라지게 된다.

그러므로 이미지의 가장자리에 0값을 갖는 픽셀을 추가한다. 

<br><br>

## Pooling layer

Pooling layer은 적당히 크기를 줄여주며, 특징을 강조하는 역할이다.

Max pooling, Average Pooling, Min Pooling이 있다.

주로 Max pooling을 사용하지만 Average Pooling도 많이 사용한다고 알고있다.

![max pooling](https://user-images.githubusercontent.com/76985302/175929246-f440bc6b-a206-41ec-96ae-6ca88ac72d21.PNG)

학습시간 단축, 오버피팅 문제 완화라는 장점이 있다.

<br><br>

## Fully Connected layer
Fully Connected layer은 특징 추출 작업을 마친 후 어떤 데이터인지 분류하는 작업을 한다.

Flatten layer에서는 데이터를 일자로 펼쳐준다. 즉, 벡터 형태로 변경시켜준다.

Softmax layer에서는 Classification을 수행한다. 최종 결과물이 나오게 된다.

<br><br>

## 2. RNN

<br>

## RNN이란?
시간의 흐름이 있는 Sequence 데이터를 다루는 신경망이다.

맥락을 이해하고 처리하는 과정에서 이전의 결과가 다음 결과에 영향을 미칠 수 있어야한다.

CNN에서는 이러한 맥락을 이해하지 못하기 때문에 RNN이 필요하다.

음성, 동영상, 주식 시세와 같은 series 데이터를 처리할 수 있다.

<br><br>

## RNN 구조
CNN과 같은 신경망은 은닉층에서 활성화 함수를 지나면 출력층의 방향으로만 향했다.

하지만 RNN에서는 은닉층에서 활성화 함수를 지난 결과 값을 출력층 방향과 은닉층의 노드의 다음 계산의 입력으로도 보낸다.

메모리 셀이라고 불리는 부분은 이전 시점의 hidden state를 다음 시점의 hidden state를 위한 입력값으로 사용한다.

일 대 다, 다 대 일, 다 대 다 등의 다양한 용도로 사용될 수 있다.

<br>

은닉층 에서는 주로 tanh를 사용한다.

<br><br>

## 깊은 순환 신경망(DRNN)
다수의 은닉층을 가진 RNN이다. 

<br><br>

## 양방향 순환 신경망(BRNN)
다음 시점이 현재 시점의 힌트가 될 수 있다.

그러므로 과거와 미래를 모두 고려해 다음 시점의 step에서 들어오는 값 또한 입력 값으로 영향을 받는다.














<br><br>

---
- 참고문헌

<https://sonsnotation.blogspot.com/2020/11/7-convolutional-neural-networkcnn.html>

<https://rubber-tree.tistory.com/116>

<https://halfundecided.medium.com/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-cnn-convolutional-neural-networks-%EC%89%BD%EA%B2%8C-%EC%9D%B4%ED%95%B4%ED%95%98%EA%B8%B0-836869f88375>

