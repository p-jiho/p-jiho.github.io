---
layout : post
title : LSTM 와 GRU
date: 2022-06-28
catagories : 
  - Deep learning
tags : 
  - Deep learning
last_modified_at: 2022-06-28
---

## 1. LSTM

## RNN의 문제점
RNN은 짧은 시퀀스에 대해서만 효과를 보인다.

CNN 부분에서 tanh를 활성화함수로 사용하는 것은 기울기 소실(vanishing gradient)가 있을 수 있다고 했다.

RNN에서는 tanh를 활성화 함수로 사용하기 때문에 장기 의존성 문제가 있을 수 있다.

이로 인해 앞쪽에 있는 정보는 거의 의미가 없어질지도 모른다.

## LSTM 이란?
RNN과 달리 불필요한 기억을 지우고, 중요한 내용을 기억하는 방식이다.

입력 게이트, 망각 게이트, 출력 게이트로 이루어져 RNN에서 망각 게이트가 추가된 모습이다.

<br>

입력게이트는 현재 시점의 입력값, 이전 시점의 은닉 상태에 가중치를 곱한 값에 시그모이드 함수와 하이퍼볼릭탄젠트 함수를 지난다.

망각게이트는 sigmoid 함수를 지나 0에 가까우면 거의 삭제, 1에 가까우면 거의 보존하는 것이다.

셀 상태는 이전 시점의 셀 상태와 망각 게이트의 출력값을 곱하고, 입력게이트의 두 출력값을 곱한 것을 더한다.

출력 게이트는 현재 입력값과 이전 시점의 은닉상태에 가중치를 곲한 값에 시그모이드 함수를 적용한다. 그 값을 셀 상태가 하이퍼볼릭탄젠트 함수를 지난 출력값과 곱해져 최종 은닉 상태가 된다.

마지막 출력층에서는 softmax 함수를 사용한다.

<br><br>

## 2. GRU

## LSTM의 문제
RNN에 비하여 파라미터가 많이 필요하다.

데이터가 충분하지 않은 경우에는 오버피팅이 발생할 수 있다.

<br><br>

## GRU란?
RNN의 장기 의존성 문제를 해결하면서 LSTM에 비해 파라미터 수가 적은 모델이다.

<br><br>

## GRU의 구조
reset 게이트, update 게이트 총 두 개의 gate를 사용한다.

장기인 cell state와 단기인 hidden state가 합쳐져 하나의 hidden state로 표현이 된다.

update 게이트가 망각 게이트와 입력 게이트를 대신한다.

update 게이트는 과거와 현재 정보를 얼마나 반영할지에 대한 비율을 구한다.

즉, 현재의 정보를 얼마나 반영할지 + 과거의 정보를 얼마나 반영할지의 출력값을 hidden state로 활용한다. 현재 정보를 많이 사용하면 과거의 정보는 적게 사용하게 된다.

reset 게이트는 sigmoid 함수를 사용하여 이전 시점의 hidden state 값을 얼마나 활용할 것인지를 계산한다.




<br><br>

---

- 참고문헌

<https://yjjo.tistory.com/17>

<https://wikidocs.net/22888>

<https://ratsgo.github.io/natural%20language%20processing/2017/03/09/rnnlstm/>

<https://hyen4110.tistory.com/26>

<https://blog.naver.com/winddori2002/221992543837>

